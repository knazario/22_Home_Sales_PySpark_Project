# 22_Home_Sales_PySpark_Project

## Overview
This project invoveld initiating a PySpark session in order to read-in a relaively large dataset (over 33,000 records) and running several SQL-based queries in order to answer questions about the data. The dataset included home sale records from 2010-2017, with various features/data points about each sale (data built, price, square footage, a view rating...etc). Questions answered included average price of various home types grouped by year, as well as average price grouped by 'View Rating'. Specific queries can be found in the notebook. 

Additionally, the same query was run several times, with slightly differnet parameters in order to see the processing time differece for each condition. The 3 conditons that were compared were: 
1. Standard ```spark.sql``` query with no modicfications
2. Caching the query-targeted table prior to running the query
3. Partitioning the data by ```date_built```, which was not a key feature of the query, and then running the query based on the partitioned temporary view


## Key Takeaways/Findings 

* Caching the data table you are running the queries on will speed up your processing time

* Partitioning can be useful, but may actually be counterproductive if the queries you are running are not utilizing the partitioned data effectively. In this example, the partioned query took the longest of the 3 conditions. The query still needed to access each of the partitions and then group data based on ```view``` and partioning by build year did not make that query more efficient. 


## Data Reference
Data for this dataset was generated by edX Boot Camps LLC, and is intended for educational purposes only.

## Code Source

No additional code sources were utilized for this project